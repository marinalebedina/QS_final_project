{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download tweets which includes specified symbol \n",
    "**\"\\$\"** which is reserved in Twitter for denomination of financial information related to stocks, bonds and currency pairs.\n",
    "\n",
    "Standard Twitter Developer API was used for this purpose. Standard API provides user with ability to download tweets corresponding to the last 10 days only.\n",
    "\n",
    "Python script for tweets’ collection was based on ”Twython”, which is a Python wrapper.\n",
    "Tweets related to each currency pair were collected in the following way:\n",
    "\n",
    "- a unique ID number of the tweet which was downloaded at the very end of the previous\n",
    "downloading session is read, and search of new tweets starts from this ID number;\n",
    "\n",
    "- searching of tweets related to currency pairs of interest is based on finding the symbol ”$” in\n",
    "tweets which is followed by name of currency pair. It a so-called financial tagging in Twitter.\n",
    "\n",
    "Right after each\n",
    "tweet was downloaded, it went through the procedure of linguistic processing, which involved the\n",
    "following:\n",
    "\n",
    "1. non-ASCII characters were removed;\n",
    "\n",
    "2. symbols representing hashtags were removed (”#”), tags together with words following this tag were removed (”@”);\n",
    "\n",
    "3. all letters were converted to lowercase;\n",
    "\n",
    "4. all URLs and links were removed;\n",
    "\n",
    "5. tweets which contain references to multiple currency pairs were removed;\n",
    "\n",
    "6. all numeric information contained in tweets were removed;\n",
    "\n",
    "7. stopwords were removed, i.e. words such as ”a”, ”the”, ”and”, ”so” etc. as these words\n",
    "usually represent noise. \n",
    "\n",
    "8. correction of misspellings was done: shortened words such as ”sho”, ”lo” were converted\n",
    "back to original words ”short” and ”long” respectively. It was important for construction of\n",
    "reliable probability model so that these words are identified as the same ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from socket import error as SocketError\n",
    "import tweepy # tool for tweets extraction\n",
    "import nltk, pprint, string, csv\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "import ast, pickle, random  \n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.isdir(\"Data/Testing_data/\"):\n",
    "    os.makedirs(\"Data/Testing_data/\")\n",
    "\n",
    "def tweets_collection(curr_pair, start_date, end_date):\n",
    "    curr_pair_ = \"'\" + curr_pair + \"'\"\n",
    "    consumer_key = \"WkmvAytt1DEWva2VukcqACVtK\"\n",
    "    consumer_secret = \"jy8D7yESo1KrUc0EgIXwVWw4IUHvSy2AgQzsMIXMUb6UtM0S9p\"\n",
    "    access_token = \"447676855-FbQsPAuLttxllF8TB4eK6CV8keYZk7BEW6UCQDuw\"\n",
    "    access_secret = \"NaFfyZMKHCP9zCpm88PcoGvOh7ZQNP3kUlPaxUjS44Vxz\"\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    def removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)\n",
    "    \n",
    "    pairs = ['Pairs', curr_pair_]\n",
    "        \n",
    "    #read control_maximum file and get maximum id to use as a filter\n",
    "    try:\n",
    "        control = open(\"Data/Testing_data/maximum_id_\"+curr_pair+\".csv\", \"r\")\n",
    "        maxid = control.readline()\n",
    "        control.close()\n",
    "    except Exception as fileEx:\n",
    "        maxid=\"0\"\n",
    "        fileToWriteError=open(\"Errors.txt\",\"a\")\n",
    "        fileToWriteError.write(\"maxid: \" + str(fileEx) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")\n",
    "\n",
    "    twitter_search = twitter.Twitter(domain='api.twitter.com', api_version='1.1',  \n",
    "                                     auth=twitter.oauth.OAuth('447676855-FbQsPAuLttxllF8TB4eK6CV8keYZk7BEW6UCQDuw', \n",
    "                                                              'NaFfyZMKHCP9zCpm88PcoGvOh7ZQNP3kUlPaxUjS44Vxz', \n",
    "                                                              'WkmvAytt1DEWva2VukcqACVtK', \n",
    "                                                              'jy8D7yESo1KrUc0EgIXwVWw4IUHvSy2AgQzsMIXMUb6UtM0S9p'))\n",
    "\n",
    "    #hold the query results\n",
    "    tweets_ids = []\n",
    "    tweets = []\n",
    "    timestamp = []\n",
    "    tweets_users = []\n",
    "\n",
    "    #run twitter search api\n",
    "    try:    \n",
    "        for tweet in tweepy.Cursor(api.search,\n",
    "                               q=curr_pair_,\n",
    "                               since=start_date,\n",
    "                               until=end_date,\n",
    "                               count = 100,\n",
    "                               lang=\"en\").items():\n",
    "            tweets_ids.append(tweet.id), \n",
    "            tweets.append(tweet.text),\n",
    "            timestamp.append(str(tweet.created_at)),\n",
    "            tweets_users.append(tweet.user.name)\n",
    "\n",
    "    except ValueError:\n",
    "        fileToWriteErrorConnection=open(\"Errors.txt\",\"a\")\n",
    "        fileToWriteErrorConnection.write(\"twitter_res append: \" + str(ValueError) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")\n",
    "\n",
    "    pairs = ['Pairs', curr_pair]\n",
    "\n",
    "    for counter in range(0, len(tweets)):\n",
    "        #remove hashtags, user tags and user retweets\n",
    "        tweets[counter]=re.sub(r'(?<=#)\\w+| #|(?<=@)\\w+| @ |(?<=RT)\\w+|RT',\"\", tweets[counter])\n",
    "        #convert tweet to lowecase \n",
    "        tweets[counter] = tweets[counter].lower()\n",
    "        tweets[counter]= re.sub(r'[^\\w:.$]',\" \",tweets[counter])\n",
    "        #remove links\n",
    "        tweets[counter] = re.sub(r'http\\\\w+',\"\",tweets[counter])\n",
    "        tweets[counter]= re.sub(r'http:.{15}',\"\",tweets[counter])\n",
    "        tweets[counter]= re.sub(r'http:.{10}',\"\",tweets[counter])\n",
    "\n",
    "        for pairCounter in range(1, len(pairs)):\n",
    "            currentPair = pairs[pairCounter].replace('/',\"\").replace('\\r\\n',\"\").lower()\n",
    "            if currentPair in tweets[counter]:\n",
    "                if not os.path.isdir(\"Data/Testing_data/\"+currentPair.upper()):\n",
    "                    os.makedirs(\"Data/Testing_data/\"+currentPair.upper())\n",
    "                try:\n",
    "                    fileToWrite = open(\"Data/Testing_data/\"+currentPair.upper()+\"/\"+currentPair.upper()+\"TemporaryRaw\"+str(start_date)+'.csv',\"a\")\n",
    "                except Exception as fileEx:\n",
    "                    fileToWriteError=open(\"Errors.txt\",\"a\")\n",
    "                    fileToWriteError.write(\"tweetWrite: \" + str(fileEx) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")\n",
    "                fileToWrite = open(\"Data/Testing_data/\"+currentPair.upper()+\"/\"+currentPair.upper()+\"TemporaryRaw\"+str(start_date)+'.csv',\"a\")\n",
    "                tweets_users[counter] = removeNonAscii(tweets_users[counter])\n",
    "                tweets[counter] = removeNonAscii(tweets[counter])\n",
    "                fileToWrite.write(str(tweets_ids[counter]) +\",\"+tweets_users[counter]+\",\"\n",
    "                                      +tweets[counter]+\",\"+time.strftime(timestamp[counter])+\"\\n\")\n",
    "                fileToWrite.close()\n",
    "\n",
    "    print ('Collected: ' + str(len(tweets_ids)) + ' tweet(s)')\n",
    "            \n",
    "    if len(tweets_ids) > 0:\n",
    "        controlIdFile = open(\"Data/Testing_data/maximum_id_\"+curr_pair+\".csv\",\"w\")\n",
    "        try:\n",
    "            controlIdFile.write(str(max(tweets_ids)))\n",
    "            controlIdFile.close()\n",
    "        except Exception as fileEx:\n",
    "            fileToWriteError=open(\"Errors.txt\",\"a\")\n",
    "            fileToWriteError.write(\"maxID writeback: \" + str(fileEx) +\",\"+datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-01-07 2019-01-08 2019-01-09 2019-01-11 2019-01-12 2019-01-13 2019-01-14 2019-01-10 2019-01-15 2019-01-16 2019-01-17 2019-01-18 2019-01-19 2019-01-20 2019-01-21 2019-01-22 2019-01-23 2019-01-24 2019-01-25 2019-01-26 2019-01-27 2019-01-28 2019-01-29 2019-01-30 2019-01-31 2019-02-01 2019-02-02 2019-02-03 2019-02-04 2019-02-05 2019-02-06 2019-02-07 2019-02-08 2019-02-09 2019-02-10 2019-02-11 2019-02-12 2019-02-13 2019-02-14 2019-02-15 2019-02-16 2019-02-17 2019-02-18 2019-02-19 2019-02-20 2019-02-21 2019-02-22 2019-02-23 2019-02-24 2019-02-25 2019-02-26 2019-02-27 2019-02-28 2019-03-01 2019-03-02 2019-03-03 2019-03-04 2019-03-05 2019-03-06 2019-03-07 2019-03-08 2019-03-09 2019-03-10 2019-03-11 2019-03-12 2019-03-13 2019-03-14 2019-03-15 2019-03-16 2019-03-17 2019-03-18 2019-03-19 2019-03-20 2019-03-21 2019-03-22 2019-03-23 2019-03-24 2019-03-25 2019-03-26 2019-03-27 2019-03-28 2019-03-29 2019-03-30 2019-03-31 2019-04-01 2019-04-02 2019-04-03 2019-04-04 2019-04-05 2019-04-06 2019-04-07 2019-04-08 2019-04-09 2019-04-10 2019-04-11 2019-04-12 2019-04-13 2019-04-14 2019-04-15 2019-04-16 2019-04-17 2019-04-18 2019-04-19 2019-04-20 2019-04-21 2019-04-22 2019-04-23 2019-04-24 2019-04-25 2019-04-26 2019-04-27 2019-04-28 2019-04-29 2019-04-30 2019-05-01 2019-05-02 2019-05-03 2019-05-04 2019-05-05 2019-05-06 2019-05-07 2019-05-08 2019-05-09 2019-05-10 2019-05-11 2019-05-12 2019-05-13 2019-05-14 2019-05-15 2019-05-16 2019-05-17 2019-05-18 2019-05-19 2019-05-20 2019-05-21 2019-05-22 2019-05-23 2019-05-24 2019-05-25 2019-05-26 2019-05-28 2019-05-29 2019-05-30 2019-05-31 2019-06-01 2019-06-02 2019-06-03 2019-06-04 2019-05-27 2019-06-05 2019-06-06 ']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600656dfbdde4675855829119f86d888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Tweet collection', max=200, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run tweets collection for 2020-05-23 EURUSD\n",
      "Collected: 268 tweet(s)\n",
      "Run tweets collection for 2020-05-23 GBPUSD\n",
      "Collected: 186 tweet(s)\n",
      "Run tweets collection for 2020-05-23 USDJPY\n",
      "Collected: 131 tweet(s)\n",
      "Run tweets collection for 2020-05-23 USDCAD\n",
      "Collected: 130 tweet(s)\n",
      "Run tweets collection for 2020-05-23 USDCHF\n",
      "Collected: 91 tweet(s)\n",
      "Run tweets collection for 2020-05-24 EURUSD\n",
      "Collected: 610 tweet(s)\n",
      "Run tweets collection for 2020-05-24 GBPUSD\n",
      "Collected: 445 tweet(s)\n",
      "Run tweets collection for 2020-05-24 USDJPY\n",
      "Collected: 351 tweet(s)\n",
      "Run tweets collection for 2020-05-24 USDCAD\n",
      "Collected: 295 tweet(s)\n",
      "Run tweets collection for 2020-05-24 USDCHF\n",
      "Collected: 172 tweet(s)\n",
      "Run tweets collection for 2020-05-25 EURUSD\n",
      "Collected: 1498 tweet(s)\n",
      "Run tweets collection for 2020-05-25 GBPUSD\n",
      "Collected: 919 tweet(s)\n",
      "Run tweets collection for 2020-05-25 USDJPY\n",
      "Collected: 655 tweet(s)\n",
      "Run tweets collection for 2020-05-25 USDCAD\n",
      "Collected: 445 tweet(s)\n",
      "Run tweets collection for 2020-05-25 USDCHF\n",
      "Collected: 361 tweet(s)\n",
      "Run tweets collection for 2020-05-26 EURUSD\n",
      "Collected: 2113 tweet(s)\n",
      "Run tweets collection for 2020-05-26 GBPUSD\n",
      "Collected: 1643 tweet(s)\n",
      "Run tweets collection for 2020-05-26 USDJPY\n",
      "Collected: 773 tweet(s)\n",
      "Run tweets collection for 2020-05-26 USDCAD\n",
      "Collected: 982 tweet(s)\n",
      "Run tweets collection for 2020-05-26 USDCHF\n",
      "Collected: 394 tweet(s)\n",
      "Run tweets collection for 2020-05-27 EURUSD\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e225b217239e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mcurr_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#                 print(curr_pair, start_date, end_date)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mtweets_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-33a0c5668cef>\u001b[0m in \u001b[0;36mtweets_collection\u001b[0;34m(curr_pair, start_date, end_date)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                \u001b[0muntil\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                                lang=\"en\").items():\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mtweets_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                                         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rate limit reached. Sleeping for: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msleep_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sleep for few extra sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# if self.wait_on_rate_limit and self._reset_time is not None and \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "import datetime\n",
    "import os.path\n",
    "from tqdm import tqdm, tnrange\n",
    "\n",
    "############################### SPECIFY CURRENCY PAIRS ###############################\n",
    "curr_list = ['EURUSD', 'GBPUSD', 'USDJPY', 'USDCAD', 'USDCHF']\n",
    "\n",
    "\n",
    "############################### SPECIFY the FIRST DATE for tweets extraction ###############################\n",
    "# First day in database\n",
    "date0 = date(2020, 5, 23) # year, month, day\n",
    "\n",
    "# Today ----> download from date0 up to now (as we can download tweets only for the last 10 days from now)\n",
    "today = datetime.date.today()\n",
    "\n",
    "# Open txt file to write\n",
    "text_file = open(\"Data/Database_content.txt\", \"r\")\n",
    "lines = text_file.readlines()\n",
    "print(lines)\n",
    "text_file.close()\n",
    "\n",
    "for i in tnrange(200, desc='Tweet collection'):\n",
    "    lower_bound = date0 + timedelta(days=i)\n",
    "    upper_bound = lower_bound + timedelta(days=1)\n",
    "    \n",
    "    if today > lower_bound:\n",
    "        if str(lower_bound) not in str(lines):\n",
    "#             print('No such date --> work further to get it')\n",
    "            text_file = open(\"Data/Database_content.txt\", \"a\")\n",
    "            start_date = lower_bound\n",
    "            end_date = upper_bound\n",
    "            \n",
    "            text_file.write(str(lower_bound))\n",
    "            text_file.write(' ')\n",
    "            for c in curr_list:\n",
    "                print('Run tweets collection for {} {}'.format(start_date, c))\n",
    "                curr_pair = c\n",
    "#                 print(curr_pair, start_date, end_date)\n",
    "                tweets_collection(curr_pair, start_date, end_date)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
